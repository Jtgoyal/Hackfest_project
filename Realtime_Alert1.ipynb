{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtrzBPlg7Xmn",
        "outputId": "e0689294-7e31-4db1-a693-8bdd935729bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "HubHWL0K70q2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install twilio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2hV3a_6J7Oa",
        "outputId": "783cc463-623f-441b-ee94-41f810f919cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twilio\n",
            "  Downloading twilio-9.5.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.32.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio) (2.10.1)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.11/dist-packages (from twilio) (3.11.15)\n",
            "Collecting aiohttp-retry>=2.8.3 (from twilio)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->twilio) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->twilio) (2025.1.31)\n",
            "Downloading twilio-9.5.1-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: aiohttp-retry, twilio\n",
            "Successfully installed aiohttp-retry-2.9.1 twilio-9.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# üîÅ Replace these values with your real Supabase details:\n",
        "SUPABASE_URL = \"https://eilbotfabkrqhrwzurbs.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVpbGJvdGZhYmtycWhyd3p1cmJzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDM3OTIyOTgsImV4cCI6MjA1OTM2ODI5OH0.-u6dMJwKiL-M2J9iB4sSdHU0a5V9nzvnL10nxGwqH3A\"  # your anon key\n",
        "TABLE_NAME = \"twitterdata\"  # use actual table name, e.g., \"twitter_comments\"\n",
        "COLUMN_NAME=\"Content\"\n",
        "\n",
        "# Set up the headers\n",
        "headers = {\n",
        "    \"apikey\": SUPABASE_KEY,\n",
        "    \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Fetch data from Supabase\n",
        "url = f\"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select={COLUMN_NAME}\"\n",
        "response = requests.get(url, headers=headers)\n"
      ],
      "metadata": {
        "id": "qvgS5zk7-SNn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK--TJF3W4n4",
        "outputId": "3f61836b-9e66-4317-eca0-d58f9f431f8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "HdOOnoypXJ7K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "d = 768  # Dimension of sentence embeddings\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.train(np.random.rand(100, d).astype(\"float32\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "ChgtQ3YUwXGo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In-memory store\n",
        "processed_data = []  # List of dicts: {\"timestamp\": datetime, \"sentence\": str, \"embedding\": np.array}\n",
        "sentences = []\n",
        "sentence_embeddings = np.empty((0, d), dtype=\"float32\")\n",
        "\n",
        "\n",
        "def fetch_and_process_new_data():\n",
        "    global processed_data, index, sentences, sentence_embeddings\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(\"‚ùå Failed to fetch data:\", response.text)\n",
        "        return\n",
        "\n",
        "    data = response.json()\n",
        "    all_sentences = [row[COLUMN_NAME] for row in data if row[COLUMN_NAME] is not None]\n",
        "\n",
        "    # Filter only new ones\n",
        "    new_sentences = all_sentences[len(processed_data):]\n",
        "    print(f\"üÜï New comments fetched: {len(new_sentences)}\")\n",
        "\n",
        "    if new_sentences:\n",
        "        new_embeddings = model.encode(new_sentences).astype(\"float32\")\n",
        "\n",
        "        for i, sent in enumerate(new_sentences):\n",
        "            ts = datetime.now()\n",
        "            processed_data.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"sentence\": sent,\n",
        "                \"embedding\": new_embeddings[i]\n",
        "            })\n",
        "            sentences.append(sent)\n",
        "\n",
        "        sentence_embeddings = np.vstack([sentence_embeddings, new_embeddings])\n",
        "        index.add(new_embeddings)\n",
        "\n",
        "    # Remove data older than 1 hour\n",
        "    cutoff = datetime.now() - timedelta(hours=1)\n",
        "    filtered_data = [entry for entry in processed_data if entry[\"timestamp\"] > cutoff]\n",
        "\n",
        "    if len(filtered_data) < len(processed_data):\n",
        "        print(f\"üóëÔ∏è Removed {len(processed_data) - len(filtered_data)} old records.\")\n",
        "\n",
        "    processed_data = filtered_data\n",
        "\n",
        "    # Rebuild index and sentence list\n",
        "    sentence_embeddings = np.array([entry[\"embedding\"] for entry in processed_data], dtype=\"float32\")\n",
        "    sentences = [entry[\"sentence\"] for entry in processed_data]\n",
        "    index.reset()\n",
        "    if len(sentence_embeddings) > 0:\n",
        "        index.add(sentence_embeddings)\n",
        "    d = sentence_embeddings.shape[1]\n",
        "    nlist = 50\n",
        "    quantizer = faiss.IndexFlatL2(d)\n",
        "    index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "    index.train(sentence_embeddings)\n",
        "    index.add(sentence_embeddings)\n",
        "    _, cluster_ids = index.quantizer.search(sentence_embeddings, 1)\n",
        "    cluster_map = {}\n",
        "    for i, cid in enumerate(cluster_ids[:, 0]):\n",
        "        cluster_map.setdefault(cid, []).append(i)\n",
        "    densest_cid = max(cluster_map, key=lambda k: len(cluster_map[k]))\n",
        "    members = cluster_map[densest_cid]\n",
        "    cluster_vectors = sentence_embeddings[members]\n",
        "    centroid = np.mean(cluster_vectors, axis=0, keepdims=True)\n",
        "    sims = faiss.IndexFlatL2(d)\n",
        "    sims.add(cluster_vectors)\n",
        "    _, idx = sims.search(centroid, 1)\n",
        "    print(\"üîî Densest Cluster Alert:\")\n",
        "    alert_message = sentences[members[idx[0][0]]]\n",
        "    print(alert_message)\n",
        "    short_alert = truncate_message(alert_message, 70)\n",
        "    send_sms(short_alert)\n",
        "\n"
      ],
      "metadata": {
        "id": "X8fja8FfXSxD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from twilio.rest import Client\n",
        "\n",
        "# Replace with your actual Twilio credentials\n",
        "TWILIO_ACCOUNT_SID = \"AC3700d0f22cc384eb47620d74edf4be1f\"\n",
        "TWILIO_AUTH_TOKEN = \"b3a98810e1fc66081f8720c8219d51b7\"\n",
        "TWILIO_PHONE_NUMBER = \"+19035687995\"  # Twilio trial number\n",
        "TARGET_PHONE_NUMBER =   # Your own phone number\n",
        "\n",
        "client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)\n",
        "\n",
        "def send_sms(message):\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            body=message,\n",
        "            from_=TWILIO_PHONE_NUMBER,\n",
        "            to=TARGET_PHONE_NUMBER\n",
        "        )\n",
        "        print(\"üì§ SMS sent:\", message.sid)\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Failed to send SMS:\", e)\n",
        "def truncate_message(msg, limit=70):\n",
        "    return msg[:limit] + (\"‚Ä¶\" if len(msg) > limit else \"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h3m5K-MwMzI4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if __name__ == \"__main__\":\n",
        "    for cycle in range(1):  # Simulate for 1 hour (6 cycles)\n",
        "        print(f\"\\n‚è±Ô∏è Cycle {cycle+1}: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "        fetch_and_process_new_data()\n",
        "        time.sleep(6)  # Wait for 10 minutes (600 seconds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD1vupnLXeNn",
        "outputId": "a1a6c4e7-efa4-4cbc-dae0-c40d9da69912"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚è±Ô∏è Cycle 1: 05:01:48\n",
            "üÜï New comments fetched: 771\n",
            "üîî Densest Cluster Alert:\n",
            "‡§Ü‡§ú ‡§¶‡•á‡§∂ ‡§ï‡•á ‡§è‡§ï ‡§ê‡§∏‡•á ‡§ï‡§≤‡§æ‡§ï‡§æ‡§∞ ‡§ú‡§ø‡§®‡•ç‡§π‡•ã‡§Ç‡§®‡•á ‡§π‡§∞ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§ï‡•á ‡§Æ‡§® ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§∂‡§≠‡§ï‡•ç‡§§‡§ø ‡§µ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡•ç‡§∞‡•á‡§Æ ‡§ï‡•Ä ‡§≠‡§æ‡§µ‡§®‡§æ ‡§ú‡§ó‡§æ‡§à‡•§ ‡§ê‡§∏‡•á ‡§≤‡•á‡§ú‡•á‡§Ç‡§° ‡§Ö‡§≠‡§ø‡§®‡•á‡§§‡§æ ‡§∂‡•ç‡§∞‡•Ä ‡§Æ‡§®‡•ã‡§ú ‡§ï‡•Å‡§Æ‡§æ‡§∞ ‡§ú‡•Ä (‡§≠‡§∞‡§§ ‡§ï‡•Å‡§Æ‡§æ‡§∞) ‡§ï‡•á ‡§®‡§ø‡§ß‡§® ‡§∏‡•á ‡§™‡•Ç‡§∞‡§æ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∏‡§ø‡§®‡•á‡§Æ‡§æ ‡§ú‡§ó‡§§ ‡§∂‡•ã‡§ï‡§æ‡§ï‡•Å‡§≤ ‡§π‡•à‡•§\n",
            " \n",
            "‡§Æ‡•à‡§Ç ‡§µ‡•ç‡§Ø‡§•‡§ø‡§§ ‡§π‡•É‡§¶‡§Ø ‡§∏‡•á ‡§∂‡•ã‡§ï ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•Å‡§è ‡§à‡§∂‡•ç‡§µ‡§∞ ‡§∏‡•á ‡§¶‡§ø‡§µ‡§Ç‡§ó‡§§ ‡§Ü‡§§‡•ç‡§Æ‡§æ ‡§ï‡•Ä ‡§∂‡§æ‡§Ç‡§§‡§ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ\n",
            "üì§ SMS sent: SM33f5ac00a0db7359a9f0b4c90aad01e7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk9O-V_UXryq",
        "outputId": "e723ffd6-d7d1-43dc-d378-d9c79987fabe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k8yslW0Xvnq",
        "outputId": "b0f3ab4a-4c2f-47e1-d0d3-8e3acb0dd61e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_jUHXJNXzOH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAIGf9-aX3_F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgwdLioDYAMs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d0kcLPPYOv6",
        "outputId": "ed28a7e9-c5d3-4b2b-e700-fb6ade4493e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîî Densest Cluster Alert:\n",
            "IPL 2025: Rishabh Pant-led LSG Stun MI, Hardik Pandya's Failed Antics In The Last-Over Deny Mumbai Much-Needed Victory#IPL2025 #RishabhPant #HardikPandya #LSGvsMI #IPL #LucknowSuperGiants #MumbaiIndians\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feJoQ5GtYSxN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvqXMmhBYWWo"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}
