{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtrzBPlg7Xmn",
        "outputId": "e5bf1d22-9b66-45be-a044-038197d73378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psycopg2-binary\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m171.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.9.10\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "HubHWL0K70q2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# ğŸ” Replace these values with your real Supabase details:\n",
        "SUPABASE_URL = \"https://eilbotfabkrqhrwzurbs.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVpbGJvdGZhYmtycWhyd3p1cmJzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDM3OTIyOTgsImV4cCI6MjA1OTM2ODI5OH0.-u6dMJwKiL-M2J9iB4sSdHU0a5V9nzvnL10nxGwqH3A\"  # your anon key\n",
        "TABLE_NAME = \"twitterdata\"  # use actual table name, e.g., \"twitter_comments\"\n",
        "COLUMN_NAME=\"Content\"\n",
        "\n",
        "# Set up the headers\n",
        "headers = {\n",
        "    \"apikey\": SUPABASE_KEY,\n",
        "    \"Authorization\": f\"Bearer {SUPABASE_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Fetch data from Supabase\n",
        "url = f\"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select={COLUMN_NAME}\"\n",
        "response = requests.get(url, headers=headers)\n"
      ],
      "metadata": {
        "id": "qvgS5zk7-SNn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK--TJF3W4n4",
        "outputId": "724b5818-46a2-41d2-d9ce-29a2a854d9b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss"
      ],
      "metadata": {
        "id": "HdOOnoypXJ7K"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "d = 768  # Dimension of sentence embeddings\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.train(np.random.rand(100, d).astype(\"float32\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "ChgtQ3YUwXGo"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In-memory store\n",
        "processed_data = []  # List of dicts: {\"timestamp\": datetime, \"sentence\": str, \"embedding\": np.array}\n",
        "sentences = []\n",
        "sentence_embeddings = np.empty((0, d), dtype=\"float32\")\n",
        "import numpy as np\n",
        "\n",
        "def fetch_and_process_new_data():\n",
        "    global processed_data, index, sentences, sentence_embeddings\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(\"âŒ Failed to fetch data:\", response.text)\n",
        "        return\n",
        "\n",
        "    data = response.json()\n",
        "    all_sentences = [row[COLUMN_NAME] for row in data if row[COLUMN_NAME] is not None]\n",
        "\n",
        "    # Filter only new ones\n",
        "    new_sentences = all_sentences[len(processed_data):]\n",
        "    print(f\"ğŸ†• New comments fetched: {len(new_sentences)}\")\n",
        "\n",
        "    if new_sentences:\n",
        "        new_embeddings = model.encode(new_sentences).astype(\"float32\")\n",
        "\n",
        "        for i, sent in enumerate(new_sentences):\n",
        "            ts = datetime.now()\n",
        "            processed_data.append({\n",
        "                \"timestamp\": ts,\n",
        "                \"sentence\": sent,\n",
        "                \"embedding\": new_embeddings[i]\n",
        "            })\n",
        "            sentences.append(sent)\n",
        "\n",
        "        sentence_embeddings = np.vstack([sentence_embeddings, new_embeddings])\n",
        "        index.add(new_embeddings)\n",
        "\n",
        "    # Remove data older than 1 hour\n",
        "    cutoff = datetime.now() - timedelta(hours=1)\n",
        "    filtered_data = [entry for entry in processed_data if entry[\"timestamp\"] > cutoff]\n",
        "\n",
        "    if len(filtered_data) < len(processed_data):\n",
        "        print(f\"ğŸ—‘ï¸ Removed {len(processed_data) - len(filtered_data)} old records.\")\n",
        "\n",
        "    processed_data = filtered_data\n",
        "\n",
        "    # Rebuild index and sentence list\n",
        "    sentence_embeddings = np.array([entry[\"embedding\"] for entry in processed_data], dtype=\"float32\")\n",
        "    sentences = [entry[\"sentence\"] for entry in processed_data]\n",
        "    index.reset()\n",
        "    if len(sentence_embeddings) > 0:\n",
        "        index.add(sentence_embeddings)\n",
        "    d = sentence_embeddings.shape[1]\n",
        "    nlist = 50\n",
        "    quantizer = faiss.IndexFlatL2(d)\n",
        "    index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
        "    index.train(sentence_embeddings)\n",
        "    index.add(sentence_embeddings)\n",
        "    _, cluster_ids = index.quantizer.search(sentence_embeddings, 1)\n",
        "    cluster_map = {}\n",
        "    for i, cid in enumerate(cluster_ids[:, 0]):\n",
        "        cluster_map.setdefault(cid, []).append(i)\n",
        "    densest_cid = max(cluster_map, key=lambda k: len(cluster_map[k]))\n",
        "    members = cluster_map[densest_cid]\n",
        "    cluster_vectors = sentence_embeddings[members]\n",
        "    centroid = np.mean(cluster_vectors, axis=0, keepdims=True)\n",
        "    sims = faiss.IndexFlatL2(d)\n",
        "    sims.add(cluster_vectors)\n",
        "    _, idx = sims.search(centroid, 1)\n",
        "    print(\"ğŸ”” Densest Cluster Alert:\")\n",
        "    print(sentences[members[idx[0][0]]])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X8fja8FfXSxD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if __name__ == \"__main__\":\n",
        "    for cycle in range(1):  # Simulate for 1 hour (6 cycles)\n",
        "        print(f\"\\nâ±ï¸ Cycle {cycle+1}: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "        fetch_and_process_new_data()\n",
        "        time.sleep(600)  # Wait for 10 minutes (600 seconds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "WD1vupnLXeNn",
        "outputId": "e1e7f3cb-6a85-4a51-e3a9-52afbf7dcc52"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â±ï¸ Cycle 1: 22:15:39\n",
            "ğŸ†• New comments fetched: 0\n",
            "ğŸ”” Densest Cluster Alert:\n",
            "IPL 2025: Rishabh Pant-led LSG Stun MI, Hardik Pandya's Failed Antics In The Last-Over Deny Mumbai Much-Needed Victory#IPL2025 #RishabhPant #HardikPandya #LSGvsMI #IPL #LucknowSuperGiants #MumbaiIndians\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2d2fe60211b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nâ±ï¸ Cycle {cycle+1}: {datetime.now().strftime('%H:%M:%S')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfetch_and_process_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for 10 minutes (600 seconds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk9O-V_UXryq",
        "outputId": "e723ffd6-d7d1-43dc-d378-d9c79987fabe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k8yslW0Xvnq",
        "outputId": "b0f3ab4a-4c2f-47e1-d0d3-8e3acb0dd61e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "252"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_jUHXJNXzOH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAIGf9-aX3_F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgwdLioDYAMs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d0kcLPPYOv6",
        "outputId": "ed28a7e9-c5d3-4b2b-e700-fb6ade4493e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”” Densest Cluster Alert:\n",
            "IPL 2025: Rishabh Pant-led LSG Stun MI, Hardik Pandya's Failed Antics In The Last-Over Deny Mumbai Much-Needed Victory#IPL2025 #RishabhPant #HardikPandya #LSGvsMI #IPL #LucknowSuperGiants #MumbaiIndians\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "feJoQ5GtYSxN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xvqXMmhBYWWo"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}