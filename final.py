# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FpXePYnWB9Zr5j5IwQ7aG6cmYyxj4-4f
"""

# !pip install psycopg2-binary

# pip install psycopg2-binary pandas requests deep-translator langdetect

import pandas as pd
import requests
import re
from langdetect import detect
from deep_translator import GoogleTranslator

# ==== üîÅ Supabase Configuration ====
SUPABASE_URL = "https://eilbotfabkrqhrwzurbs.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVpbGJvdGZhYmtycWhyd3p1cmJzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDM3OTIyOTgsImV4cCI6MjA1OTM2ODI5OH0.-u6dMJwKiL-M2J9iB4sSdHU0a5V9nzvnL10nxGwqH3A"
TABLE_NAME = "twitterdata"
COLUMN_NAMES = "id,Timestamp,Content"

headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

# ==== üì• Step 1: Fetch Data from Supabase ====
url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select={COLUMN_NAMES}"
response = requests.get(url, headers=headers)

if response.status_code != 200:
    raise Exception(f"Failed to fetch data from Supabase: {response.text}")

data = response.json()
df = pd.DataFrame(data)

def remove_special_characters(text):
    return re.sub(r'[^a-zA-Z\s]', '', text)

def is_hinglish(text):
    hindi_words = ['mera', 'tum', 'kya', 'nahi', 'hai', 'tha', 'ho', 'hain']
    text_lower = text.lower()
    return any(word in text_lower for word in hindi_words)

def clean_and_translate(text):
    cleaned = remove_special_characters(text).strip()

    if len(cleaned.split()) < 3:  # Not enough for lang detection
        return None

    try:
        lang = detect(cleaned)
    except:
        return None

    if lang == 'en':
        if is_hinglish(cleaned):
            try:
                return GoogleTranslator(source='auto', target='en').translate(cleaned)
            except:
                return None
        return cleaned  # Pure English
    else:
        return None  # Not English/Hinglish

# ==== Step 3: Process Each Row ====
for index, row in df.iterrows():
    original_id = row["id"]
    content = row["Content"]

    updated_text = clean_and_translate(content)

    if updated_text is None or updated_text.strip() == "":
        # Delete from Supabase
        delete_url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?id=eq.{original_id}"
        del_res = requests.delete(delete_url, headers=headers)

    else:
        # Update back into Supabase
        update_url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?id=eq.{original_id}"
        payload = {
            "Content": updated_text
        }
        up_res = requests.patch(update_url, headers=headers, json=payload)

import pandas as pd
import requests
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# ==== Setup Sentiment Tools ====
nltk.download('vader_lexicon')

vader_analyzer = SentimentIntensityAnalyzer()
model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
roberta_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# ==== Define Sentiment Classification Functions ====
def classify_with_vader(text):
    scores = vader_analyzer.polarity_scores(text)
    compound = scores['compound']
    if compound >= 0.5:
        return 'Positive'
    elif compound <= -0.5:
        return 'Negative'
    else:
        return 'Neutral'

def classify_with_roberta(text):
    result = roberta_pipeline(text)[0]
    return result['label']  # 'LABEL_0', 'LABEL_1', or 'LABEL_2' ‚Üí now returns 'Negative', 'Neutral', 'Positive'

def combined_sentiment_analysis(text):
    vader_sentiment = classify_with_vader(text)
    if vader_sentiment == 'Neutral':
        return vader_sentiment
    else:
        roberta_sentiment = classify_with_roberta(text)
        return roberta_sentiment

# ==== Define Main Function ====
def analyze_and_update_sentiments():
    # Step 1: Fetch Data
    url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select=id,Content"
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        raise Exception(f"Failed to fetch data: {response.text}")

    data = response.json()
    df = pd.DataFrame(data)

    # Step 2: Loop through and update sentiments
    for _, row in df.iterrows():
        record_id = row["id"]
        text = row["Content"]

        # try:
        sentiment = combined_sentiment_analysis(text)

            # Update record in Supabase
        update_url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?id=eq.{record_id}"
        payload = {"Sentiment": sentiment}
        patch = requests.patch(update_url, headers=headers, json=payload)

        #     if patch.status_code in [200, 204]:
        #         print(f"‚úÖ ID {record_id}: Sentiment updated to '{sentiment}'")
        #     else:
        #         print(f"‚ùå ID {record_id}: Failed to update ‚Üí {patch.text}")

        # except Exception as e:
        #   print(f"‚ö†Ô∏è ID {record_id}: Error processing ‚Üí {str(e)}")

# ==== Run It ====
analyze_and_update_sentiments()

import matplotlib.pyplot as plt

COLUMN_NAMES = "Sentiment"
headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

# Fetch only Sentiment column
url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select={COLUMN_NAMES}"
response = requests.get(url, headers=headers)

if response.status_code != 200:
    raise Exception(f"Failed to fetch data from Supabase: {response.text}")

# Load data into DataFrame
data = response.json()
df = pd.DataFrame(data)

# Drop missing or empty sentiments
df = df[df['Sentiment'].notna() & (df['Sentiment'] != '')]
df['Sentiment'] = df['Sentiment'].str.strip().str.capitalize()
# Count sentiment categories
sentiment_counts = df['Sentiment'].value_counts()

# Plot pie chart
plt.figure(figsize=(6, 6))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=['green', 'gray', 'red'])
plt.title("Sentiment Distribution")
plt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle.
plt.show()

# ==== Load Emotion Model ====
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
import requests
import pandas as pd
import torch

# Model
model_name = "cardiffnlp/twitter-roberta-base-emotion"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Label mapping (based on model config)
emotion_labels = ['anger', 'joy', 'optimism', 'sadness']

# ==== Define Emotion Classification Function ====
def detect_emotion(text):
    encoded_input = tokenizer(text, return_tensors='pt', truncation=True)
    with torch.no_grad():
        output = model(**encoded_input)
    scores = softmax(output.logits.numpy()[0])
    return emotion_labels[scores.argmax()]

# ==== Define Main Function ====
def analyze_and_update_emotions():
    # Step 1: Fetch Data
    url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select=id,Content"
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        raise Exception(f"Failed to fetch data: {response.text}")

    data = response.json()
    df = pd.DataFrame(data)

    # Step 2: Loop through and update emotions
    for _, row in df.iterrows():
        record_id = row["id"]
        text = row["Content"]

        # try:
        emotion = detect_emotion(text)

            # Update record in Supabase
        update_url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?id=eq.{record_id}"
        payload = {"Emotion": emotion}
        patch = requests.patch(update_url, headers=headers, json=payload)

        #     if patch.status_code in [200, 204]:
        #         print(f"‚úÖ ID {record_id}: Emotion ‚Üí {emotion}")
        #     else:
        #         print(f"‚ùå ID {record_id}: Failed to update ‚Üí {patch.text}")

        # except Exception as e:
        #     print(f"‚ö†Ô∏è ID {record_id}: Error processing ‚Üí {str(e)}")

analyze_and_update_emotions()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import requests

# === Step 1: Fetch Emotion Data from Supabase ===
url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select=Emotion"
response = requests.get(url, headers=headers)

if response.status_code != 200:
    raise Exception(f"Failed to fetch data: {response.text}")

df = pd.DataFrame(response.json())

# === Step 2: Count Each Emotion ===
emotion_counts = df['Emotion'].value_counts().reset_index()
emotion_counts.columns = ['Emotion', 'Count']
emotion_counts['Count'] = emotion_counts['Count'].astype(int)  # ‚úÖ Fix here

# === Step 3: Convert to Pivot Table for Heatmap ===
heat_df = emotion_counts.pivot_table(index='Emotion', values='Count')

# === Step 4: Plot Heatmap ===
plt.figure(figsize=(6, 5))
sns.heatmap(
    heat_df,
    annot=True,
    cmap="Reds",
    linewidths=0.5,
    fmt=".0f",  # Works now
    cbar=True
)

plt.title("Current Emotion Intensity")
plt.xlabel("Emotion")
plt.ylabel("")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# !pip install psycopg2-binary

import psycopg2
import pandas as pd

# pip install twilio

import requests

# üîÅ Replace these values with your real Supabase details:
SUPABASE_URL = "https://eilbotfabkrqhrwzurbs.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImVpbGJvdGZhYmtycWhyd3p1cmJzIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDM3OTIyOTgsImV4cCI6MjA1OTM2ODI5OH0.-u6dMJwKiL-M2J9iB4sSdHU0a5V9nzvnL10nxGwqH3A"  # your anon key
TABLE_NAME = "twitterdata"  # use actual table name, e.g., "twitter_comments"
COLUMNS="Content,Sentiment"
COLUMN_NAME = "Content"
SENTIMENT_COLUMN = "Sentiment"



# Set up the headers
headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

# Fetch data from Supabase
url = f"{SUPABASE_URL}/rest/v1/{TABLE_NAME}?select={COLUMNS}"
response = requests.get(url, headers=headers)

# !pip install faiss-cpu

import faiss

from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer
import numpy as np
model = SentenceTransformer('bert-base-nli-mean-tokens')
d = 768  # Dimension of sentence embeddings
index = faiss.IndexFlatL2(d)
index.train(np.random.rand(100, d).astype("float32"))

# In-memory store
processed_data = []  # List of dicts: {"timestamp": datetime, "sentence": str, "embedding": np.array}
sentences = []
sentence_embeddings = np.empty((0, d), dtype="float32")


def fetch_and_process_new_data():
    global processed_data, index, sentences, sentence_embeddings

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        # print("‚ùå Failed to fetch data:", response.text)
        return

    data = response.json()
    all_new_entries = [
    {"sentence": row[COLUMN_NAME], "sentiment": row[SENTIMENT_COLUMN]}
    for row in data
    if row[COLUMN_NAME] is not None and SENTIMENT_COLUMN in row
]

    # Filter only new ones
    new_entries = all_new_entries[len(processed_data):]
    new_sentences = [entry["sentence"] for entry in new_entries]
    new_sentiments = [entry["sentiment"] for entry in new_entries]

    if new_sentences:
        new_embeddings = model.encode(new_sentences).astype("float32")

        for i, sent in enumerate(new_sentences):
            ts = datetime.now()
            processed_data.append({
                "timestamp": ts,
                "sentence": new_sentences[i],
                "sentiment": new_sentiments[i],
                "embedding": new_embeddings[i]
            })
            sentences.append(sent)

        sentence_embeddings = np.vstack([sentence_embeddings, new_embeddings])
        index.add(new_embeddings)

    # Remove data older than 1 hour
    cutoff = datetime.now() - timedelta(hours=1)
    filtered_data = [entry for entry in processed_data if entry["timestamp"] > cutoff]

    # if len(filtered_data) < len(processed_data):
        # print(f"üóëÔ∏è Removed {len(processed_data) - len(filtered_data)} old records.")

    processed_data = filtered_data

    # Rebuild index and sentence list
    sentence_embeddings = np.array([entry["embedding"] for entry in processed_data], dtype="float32")
    sentences = [entry["sentence"] for entry in processed_data]
    index.reset()
    if len(sentence_embeddings) > 0:
        index.add(sentence_embeddings)
    d = sentence_embeddings.shape[1]
    nlist = 50
    quantizer = faiss.IndexFlatL2(d)
    index = faiss.IndexIVFFlat(quantizer, d, nlist)
    index.train(sentence_embeddings)
    index.add(sentence_embeddings)
    _, cluster_ids = index.quantizer.search(sentence_embeddings, 1)
    cluster_map = {}
    for i, cid in enumerate(cluster_ids[:, 0]):
        cluster_map.setdefault(cid, []).append(i)
    densest_cid = max(cluster_map, key=lambda k: len(cluster_map[k]))
    members = cluster_map[densest_cid]
    cluster_vectors = sentence_embeddings[members]
    centroid = np.mean(cluster_vectors, axis=0, keepdims=True)
    sims = faiss.IndexFlatL2(d)
    sims.add(cluster_vectors)
    _, idx = sims.search(centroid, 1)
    print("üîî Densest Cluster Alert:")
    alert_index = members[idx[0][0]]
    alert_comment = sentences[alert_index]
    alert_sentiment = processed_data[alert_index]["sentiment"]
    short_alert = truncate_message(alert_comment, 70)
    # Route based on categorical sentiment
    if alert_sentiment == "negative":
      send_sms(short_alert)
    else:
      print(sentences[members[idx[0][0]]])

from twilio.rest import Client

# Replace with your actual Twilio credentials
TWILIO_ACCOUNT_SID = "AC3700d0f22cc384eb47620d74edf4be1f"
TWILIO_AUTH_TOKEN = "b3a98810e1fc66081f8720c8219d51b7"
TWILIO_PHONE_NUMBER = "+19035687995"  # Twilio trial number
TARGET_PHONE_NUMBER =   # Your own phone number

client = Client(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN)

def send_sms(message):
    try:
        message = client.messages.create(
            body=message,
            from_=TWILIO_PHONE_NUMBER,
            to=TARGET_PHONE_NUMBER
        )
        print("üì§ SMS sent:", message.sid)
    except Exception as e:
        print("‚ùå Failed to send SMS:", e)
def truncate_message(msg, limit=70):
    return msg[:limit] + ("‚Ä¶" if len(msg) > limit else "")

import time
if __name__ == "__main__":
    for cycle in range(1):  # Simulate for 1 hour (6 cycles)
        print(f"\n‚è±Ô∏è Cycle {cycle+1}: {datetime.now().strftime('%H:%M:%S')}")
        fetch_and_process_new_data()
        time.sleep(6)



import streamlit as st
st.set_page_config(layout="wide", page_title="NLP Dashboard")
st.title("üß† NLP Analysis Dashboard")
# @st.cache_data

st.subheader("üìä Sentiment Analysis")
fig1, ax1 = plt.subplots()
st.pyplot(fig1)
